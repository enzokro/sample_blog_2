[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLM Class Blog",
    "section": "",
    "text": "Landing page"
  },
  {
    "objectID": "blog/posts/2023-09-24-Second-Post/index.html",
    "href": "blog/posts/2023-09-24-Second-Post/index.html",
    "title": "Run a Large Language Model using the HuggingFace Transformers API.",
    "section": "",
    "text": "The cells below are good defaults for development.\nThe autoreload lines help load libraries on the fly, while they are changing. This works well with the editable install we created via pip install -e .\nThis means we can edit the source code directly and have the change reflected live in the notebook.\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline"
  },
  {
    "objectID": "blog/posts/2023-09-24-Second-Post/index.html#first-a-pipeline",
    "href": "blog/posts/2023-09-24-Second-Post/index.html#first-a-pipeline",
    "title": "Run a Large Language Model using the HuggingFace Transformers API.",
    "section": "First, a Pipeline",
    "text": "First, a Pipeline\nA HuggingFace model is based on 3 key pieces: 1. Config file.\n2. Preprocessor file.\n3. Model file.\nThe HuggingFace API gives us a way of automatically using these pieces directly: the pipeline.\nLet‚Äôs get right it and create a Sentiment Analysis pipeline.\n\n# load in the pipeline object from huggingface\nfrom transformers import pipeline\n\n# create the sentiment analysis pipeline\nclassifier = pipeline(\"sentiment-analysis\")\n\n/Users/cck/mambaforge/envs/llm_base/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDownloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 629/629 [00:00&lt;00:00, 1.06MB/s]\nDownloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 268M/268M [00:34&lt;00:00, 7.76MB/s] \nDownloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00&lt;00:00, 432kB/s]\nDownloading (‚Ä¶)solve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00&lt;00:00, 9.00MB/s]\n\n\nWe can see in the output message above that HuggingFace automatically picked a decent, default model for us since we didn‚Äôt specify one. Specifically, it chose a distilbert model.\nWe will learn more about what exactly distilbert is and how it works later on. For now, think of it as a useful NLP genie who can tell us how it feels about a given sentence.\n\n# example from the HuggingFace tutorial\nclassifier(\"We are very happy to show you the ü§ó Transformers library.\")\n\n[{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n\n\n\n# passing in several sentences at once, inside a python list\nresults = classifier([\n    \"We are very happy to show you the ü§ó Transformers library.\",\n    \"We hope you don't hate it.\",\n    \"I love Fractal! I'm so glad it's not a cult!\", \n])\n\n# print the output of each results\nfor result in results:\n    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\nlabel: POSITIVE, with score: 0.999"
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "core",
    "section": "",
    "text": "Fill in a module description here\n\n::: {.cell 0=‚Äòh‚Äô 1=‚Äòi‚Äô 2=‚Äòd‚Äô 3=‚Äòe‚Äô}\nfrom nbdev.showdoc import *\n:::\n::: {.cell 0=‚Äòe‚Äô 1=‚Äòx‚Äô 2=‚Äòp‚Äô 3=‚Äòo‚Äô 4=‚Äòr‚Äô 5=‚Äòt‚Äô}\ndef foo(): pass\n\ndef my_first_function():\n    \"\"\"Simple greeter.\"\"\"\n    print(\"Hello, world!\")\n:::\n::: {.cell 0=‚Äòh‚Äô 1=‚Äòi‚Äô 2=‚Äòd‚Äô 3=‚Äòe‚Äô}\nimport nbdev; nbdev.nbdev_export()\n:::"
  },
  {
    "objectID": "blog/posts/2023-09-24-First-Post/index.html",
    "href": "blog/posts/2023-09-24-First-Post/index.html",
    "title": "Environment setups",
    "section": "",
    "text": "Running a Large Language Model using the HuggingFace Transformers API."
  },
  {
    "objectID": "blog/posts/2023-09-24-First-Post/index.html#things-we-need-for-the-class",
    "href": "blog/posts/2023-09-24-First-Post/index.html#things-we-need-for-the-class",
    "title": "Environment setups",
    "section": "Things we need for the class",
    "text": "Things we need for the class\nIn order to fully use a current, open source LLM, the first thing we need to do is set up a proper programming environment. The environment is a computing ecosystem with all the software libraries and packages needed to run an LLM.\nNote that setting up this environment is often one of the most time-consuming and challenging tasks when it comes to Machine Learning. There is no silver bullet or universal solution, as you will see by the dozens of tools that folks have come up with to tackle this problem (insert xkcd comic about competing standards).\nThe main point here is that setting up the environment is often annoying. It can even be straight up painful. It‚Äôs ok to feel lost or struggle with it. Please take some comfort in the fact that once we have the environment, many of the downstream tasks will feel easy by comparison!\nSo what makes building this environment so challenging? And why do we need it in the first place?\n\nSilent Failures in AI Models\nLLMs, and Machine Learning models more generally, often fail in different ways than other, standard software fails. For instance, classic bugs in regular software include: type mismatches, syntax errors, compilation errors, etc. In other words failures that clearly stem from a wrong operation (aka a bug) that snuck into the code. We wanted the computer to do X, but we told it by accident to do Y instead.\nIn contrast, ML models often have ‚Äúsilent‚Äù failures. There is no syntax or compilation error - the program still runs and completes fine. But, there is something wrong in the code: adding where we should have subtracted, grabbing the wrong element from a list, or using the wrong mathematical function. There is no type checker or compiler that would (or even could, for now) catch these errors.\nThe fix for the silent failures above is clear:\n- Carefully inspecting the code.\n- Monitoring and validating its outputs.\n- Clarity in both the algorithms and models we are using.\nThere is another, unfortunate kind of silent failure: version mismatches. Version failures happen when we use a different version of a programming library than the version originally used by the model. As the software libraries we rely on are frequently updated, both subtle and major changes in their internals can affect the output of a model. These failures are unfortunately immune to our careful logical checks.\nAvoiding these silent failures is the main reason for being consistent and disciplined with our model‚Äôs programming environment. A good environment setup keeps us focused on the important, conceptual part of our model instead of getting bogged down in managing software versions.\n\n\nLooking forward with our environment\nThere is a nice benefit to spending this much time and effort up front on our environment.\nWe will not only have a specialized environment to run and fine-tune a single LLM. We‚Äôll have a springboard and setup to keep up with the state of the art in the field. A setup to bring in groundbreaking improvements as they are released. And to weave in the latest and greatest models. The LLM world is our oyster, and the base environment the grain of sand soon-to-be pearls."
  },
  {
    "objectID": "blog/posts/2023-09-24-First-Post/index.html#organizing-what-we-need",
    "href": "blog/posts/2023-09-24-First-Post/index.html#organizing-what-we-need",
    "title": "Environment setups",
    "section": "Organizing what we need",
    "text": "Organizing what we need\nThe mamba package manager will handle the python version. Why Mamba? To start it is way fast and better than Anaconda, and it makes it easier to install OS and system-level packages we need outside of python.\nWe will use pip to install the actual python packages. Note that we could use mamba for this as well, but a few of the libraries need custom pip options to install.\n\nNote: Run pip install -e . to install a dynamic version of this package that tracks live code changes."
  },
  {
    "objectID": "blog/posts/2023-09-24-First-Post/index.html#mac-installation",
    "href": "blog/posts/2023-09-24-First-Post/index.html#mac-installation",
    "title": "Environment setups",
    "section": "Mac Installation",
    "text": "Mac Installation\nFirst find the name of your architecture. We then use it to pick the right install script for each Mac.\n\n# check your mac's architecture\narch=$(uname) \necho $arch\n\n# download the appropriate installation script\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\n\n# run the Mambaforge installer\nbash Mambaforge-$(uname)-$(uname -m).sh\nIf you prefer to download the file directly, grab it from here:\nhttps://github.com/conda-forge/miniforge/releases/"
  },
  {
    "objectID": "blog/posts/2023-09-24-First-Post/index.html#creating-the-environment",
    "href": "blog/posts/2023-09-24-First-Post/index.html#creating-the-environment",
    "title": "Environment setups",
    "section": "Creating the environment",
    "text": "Creating the environment\nAfter installing Mamba, head to the Lesson 0 here: Fractal_LLM_Course/lesson_0/envs. The README.md in that folder has the full instructions to build the mamba environment."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Example Fractal-U Blog",
    "section": "",
    "text": "Environment setups\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRun a Large Language Model using the HuggingFace Transformers API.\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  }
]